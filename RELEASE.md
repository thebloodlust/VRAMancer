# VRAMancer v0.1.0 â€” Release Notes

## ğŸš€ NouveautÃ©s majeures

- **Packaging universel** :
  - Build .deb prÃªt Ã  lâ€™emploi (voir `build_deb.sh` ou `make deb`)
  - Archive portable `.tar.gz` (`make archive`)
  - Version Lite CLI only (`make lite`)

- **Backends LLM unifiÃ©s** :
  - SÃ©lection dynamique : HuggingFace, vLLM, Ollama (auto/CLI/config)
  - DÃ©coupage adaptatif selon la VRAM rÃ©elle
  - Stubs enrichis pour vLLM/Ollama (prÃªts pour intÃ©gration rÃ©elle)

- **Orchestration GPU avancÃ©e** :
  - Exploitation automatique des GPU secondaires pour monitoring, offload, orchestration, worker rÃ©seau
  - Monitoring GPU secondaire en thread (exemple inclus)

- **RÃ©seau flexible** :
  - SÃ©lection auto/manuel de lâ€™interface rÃ©seau (CLI/config)
  - Fallback intelligent si interface indisponible

- **ModularitÃ© & ExtensibilitÃ©** :
  - Dashboards multiples (Qt, Tk, Web, CLI)
  - Premium modules (dÃ©sactivables en version Lite)
  - Structure Python moderne, packaging pro, tests unitaires

## ğŸ› ï¸Â Installation rapide

```bash
git clone https://github.com/thebloodlust/VRAMancer.git
cd VRAMancer
bash Install.sh
source .venv/bin/activate
make deb           # ou make archive / make lite
```

## ğŸ–¥ï¸Â Lancement (exemples)

- **Mode auto** (backend, dashboard, GPU, rÃ©seau auto)
  ```bash
  python -m vramancer.main
  ```
- **Forcer un backend**
  ```bash
  python -m vramancer.main --backend vllm --model mistral
  ```
- **Version Lite (CLI only)**
  ```bash
  tar -xzf vramancer-lite.tar.gz
  cd vramancer-lite
  python -m vramancer.main --backend huggingface --model gpt2
  ```
- **SÃ©lection rÃ©seau manuelle**
  ```bash
  python -m vramancer.main --net-mode manual
  ```

## ğŸ”¥Â Exploitation GPU secondaires (exemple)

- Les GPU non utilisÃ©s pour lâ€™infÃ©rence principale sont dÃ©tectÃ©s automatiquement et peuvent Ãªtre utilisÃ©s pourÂ :
  - Monitoring (thread Python inclus)
  - Offload (swap, worker rÃ©seau, etc.)

## ğŸ§©Â Backends LLM (exemple dâ€™intÃ©gration)

```python
from core.backends import select_backend
backend = select_backend("auto")  # ou "huggingface", "vllm", "ollama"
model = backend.load_model("gpt2")
blocks = backend.split_model(num_gpus=2)
out = backend.infer(inputs)
```

## ğŸ“Â Changelog

- Packaging .deb/CLI lite universel
- SÃ©lection backend dynamique (auto/CLI/config)
- DÃ©coupage adaptatif VRAM
- Usages GPU secondaires (monitoring, offload)
- SÃ©lection rÃ©seau auto/manuel
- Backends vLLM/Ollama enrichis
- Version Lite CLI only
- Refactoring, docstring, robustesse accrue

---

**Projet maintenu par thebloodlust â€” contributions bienvenues !**
