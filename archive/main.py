#!/usr/bin/env python3
"""
VRAMancer Deluxe â€“ Point dâ€™entrÃ©e principal.
"""
import random          # <â€‘â€‘ Ajout
import torch

from core.model_splitter   import ModelSplitter
from core.stream_manager   import StreamManager
from core.compute_engine   import ComputeEngine
from core.transfer_manager import TransferManager
from core.memory_balancer  import MemoryBalancer
from core.scheduler        import Scheduler
from core.logger           import Logger

# Dashboards
from dashboard.updater import update_dashboard
from dashboard.app import launch_tk_dashboard  # ou launch() pour la CLI

def main():
    print("ðŸš€ VRAMancer Deluxe California Ripper â€” Initialisation")

    # 1ï¸âƒ£  Instanciation des modules mÃ©tier
    scheduler = Scheduler()
    logger    = Logger()
    splitter  = ModelSplitter("bert-base-uncased")
    streamer  = StreamManager(scheduler, logger)
    engine    = ComputeEngine(backend="auto")
    transfer  = TransferManager(protocol="vramancer", secure=True)
    balancer  = MemoryBalancer(scheduler, logger)

    gpus = scheduler.get_available_gpus()
    print(f"ðŸ§  GPUs dÃ©tectÃ©s : {[gpu['id'] for gpu in gpus]}")

    # 2ï¸âƒ£  DÃ©coupage du modÃ¨le
    allocation = splitter.split_by_gpu(gpus)
    print("ðŸ“¦ RÃ©partition des couches :")
    for gpu_id, layers in allocation.items():
        print(f"  GPU {gpu_id} â†’ {len(layers)} couches")

    # 3ï¸âƒ£  Lancement du dashboard (en parallÃ¨le)
    import threading
    t_dashboard = threading.Thread(target=launch_tk_dashboard, daemon=True)
    t_dashboard.start()

    # 4ï¸âƒ£  Simulation dâ€™un batch dâ€™entrÃ©e
    batch = torch.randn(32, 768)

    # Historique fictif pour la prÃ©diction de rÃ©Ã©quilibrage
    usage_history = {
        gpu["id"]: [random.randint(60, 95) for _ in range(5)] for gpu in gpus
    }

    # 5ï¸âƒ£  Boucle dâ€™exÃ©cution
    for gpu_id, layers in allocation.items():
        print(f"\nðŸŽ¯ Traitement sur GPU {gpu_id}")
        streamer.prefetch_layers(layers, lookahead=3)

        for layer in layers:
            # â”€â”€ 5.1  Chargement + exÃ©cution
            block  = streamer.preload_layer(layer)
            output = engine.execute_layer(
                layer, batch,
                device_id=gpu_id,
                track_gradients=True,
                use_compile=True,
                profile_gpu=True
            )

            # â”€â”€ 5.2  Synchronisation activations
            activations_map = {layer["name"]: (gpu_id, (gpu_id + 1) % len(gpus), output)}
            transfer.sync_activations(activations_map)

            # â”€â”€ 5.3  RÃ©Ã©quilibrage mÃ©moire
            balancer.balance([layer])
            balancer.predictive_balance(usage_history)

            # â”€â”€ 5.4  LibÃ©ration couche
            streamer.release_layer(layer["name"])

            # â”€â”€ 5.5  Mise Ã  jour du dashboard (mÃ©moire a changÃ©)
            update_dashboard(balancer, dashboard_module=balancer)

        # Monitoring live (si vous lâ€™avez implÃ©mentÃ©)
        streamer.live_monitoring()
        balancer.emergency_release()

    print("\nâœ… ExÃ©cution terminÃ©e â€” VRAMancer Deluxe a tout donnÃ© ðŸ’¥")


if __name__ == "__main__":
    main()
